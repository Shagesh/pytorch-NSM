# pynsm

## Features

### Similarity matching with arbitrary encoder

The code defines a neural network module called `SimilarityMatching`. This model takes an arbitrary encoder module and generates a "competitor" layer -- a linear layer that enforces competition between the different output channels from the encoder. Natural examples of encoder layers are fully-connected layers and convolutional layers. The `forward` method performs the forward pass of the model, which involves running to convergence the dynamics involving the lateral competitor connections. The model also includes a method for calculating the loss from which the plasicity rules can be derived.

### ZCA Whitening

The code includes a ZCA whitening function `computeZCAMatrix` that computes the ZCA matrix for a set of input observations `X`. The function performs normalization, reshaping, covariance computation, singular value decomposition (SVD), and builds the ZCA matrix. The whitening transformation is implemented in the `ZCATransformation` class, which takes the ZCA matrix and transformation mean as inputs and applies the transformation to a given tensor image.

### Training the Model

One of the key design goals was to make using the `SimilarityMatching` modules be as seamless as possible. Indeed, training the module can be done in precisely the same way as training any other PyTorch module. The only difference is that there is a canonical choice for the loss function, and that can be obtained from the `loss()` method of the module. See the example notebooks for details on the training.

### Classification

The pre-trained network is used in a supervised fashion to perform classification. Specifically, the embeddings generated by the `SimilarityMatching` module are followed by a max-pooling layer and then passed through an SVM (using `SGDClassifier` from `sklearn`). We obtain good accuracy on the test set.

### Supervised NSM

The code also introduces a module called `SupervisedSimilarityMatching` class, which implements supervised learning. It includes additional functionality for handling labeled data during training. It is a special case of `MultiSimilarityMatching`, which attempts to maximize the similarity between the circuit's output and multiple inputs.
